{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer\n",
    "from datasets import load_dataset\n",
    "import pyarabic.araby as araby\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset TyDiQA\n",
    "\n",
    "#### TyDiQA Gold Passage (GoldP) Task\n",
    "\n",
    "The TyDiQA Gold Passage (GoldP) task is a streamlined version of the main TyDiQA dataset designed for question answering. It focuses on answerable factual questions and provides only the relevant passage, making it compatible with existing tools and easier for researchers to get started. Importantly, this dataset is **multilingual**, covering information retrieval in eleven typologically diverse languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/anaconda3/envs/huggingface/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for tydiqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tydiqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"tydiqa\", \"secondary_task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 49881\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 5077\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the unique ids to filter the arabic ids\n",
    "uq =  {i for i in raw_dataset['train']['id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14805"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting the arabic ids and count their length\n",
    "arabic_ids = [i for i in uq if i[0:6] == \"arabic\"]\n",
    "len(arabic_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Non-Arabic Samples\n",
    "\n",
    "This code identifies non-Arabic samples in a dataset (`raw_dataset`) and stores their indices for exclusion.\n",
    "\n",
    "- Creates lists (`exclude_idx`, `exclude_idx_val`) of indices where the first 6 characters of the training/validation set's `id` field aren't \"arabic\".\n",
    "\n",
    "- `store_non_arabic_samples_to_file` function:\n",
    "    - Takes `type` (train/validation) and saves indices to a file (`{type}_non_arabic_indices.txt`).\n",
    "    - Reads the file, converts entries to integers (excluding empty elements), and returns a list of non-Arabic sample indices.\n",
    "\n",
    "This approach helps filter the dataset by storing indices of non-Arabic samples for later exclusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the indicis of the non arabic samples from the dataset to exclude them from the dataset\n",
    "exclude_idx =  [i for i in range(len(raw_dataset['train']['id'])) if raw_dataset['train']['id'][i][0:6] != \"arabic\"]\n",
    "exclude_idx_val =  [i for i in range(len(raw_dataset['validation']['id'])) if raw_dataset['validation']['id'][i][0:6] != \"arabic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_non_arabic_samples_to_file(type, ex_indices=None):\n",
    "    if os.path.exists(f'{type}_non_arabic_indices.txt') == False:\n",
    "        file = open(f'{type}_non_arabic_indices.txt','w')\n",
    "        for idx in ex_indices:\n",
    "            file.write(str(idx) + \"\\n\")\n",
    "        file.close()\n",
    "        \n",
    "\n",
    "    # opening the file in read mode \n",
    "    my_file = open(f'{type}_non_arabic_indices.txt', \"r\") \n",
    "\n",
    "    # reading the file \n",
    "    data = my_file.read() \n",
    "\n",
    "    # replacing end of line('/n') with ' ' and \n",
    "    # splitting the text it further when '.' is seen. \n",
    "    rename_data_into_list = data.replace('\\n', ' ').split(\" \") \n",
    "    rename_data_into_list = [int(i) for i in rename_data_into_list if len(i) != 0]\n",
    "    print(f\" our list length: {len(rename_data_into_list)}\")\n",
    "    my_file.close()\n",
    "    return rename_data_into_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " our list length: 35076\n",
      " our list length: 4156\n"
     ]
    }
   ],
   "source": [
    "exclude_train_indices = store_non_arabic_samples_to_file(\"train\")\n",
    "exclude_val_indices = store_non_arabic_samples_to_file(\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Arabic Subsets\n",
    "\n",
    "This code filters training/validation data (using `select`) to exclude samples identified as non-Arabic (based on `exclude_train_indices` and `exclude_val_indices`). It creates Arabic-only subsets (`train_dataset_sel`, `val_dataset_sel`) and prints their sizes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 14805 | Validating samples: 921\n"
     ]
    }
   ],
   "source": [
    "# Choosing the arabic data only from our dataset\n",
    "train_dataset_sel = raw_dataset['train'].select(\n",
    "    (\n",
    "        i for i in range(len(raw_dataset['train'])) \n",
    "        if i not in set(exclude_train_indices)\n",
    "    )\n",
    ")\n",
    "val_dataset_sel = raw_dataset['validation'].select(\n",
    "    (\n",
    "        i for i in range(len(raw_dataset['validation'])) \n",
    "        if i not in set(exclude_val_indices)\n",
    "    )\n",
    ")\n",
    "print(f\"Training samples: {len(train_dataset_sel)} | Validating samples: {len(val_dataset_sel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking that there is only one answer to each context in the dataset\n",
    "raw_dataset[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic Tokenization Setup\n",
    "\n",
    "- Sets up Arabic NER tokenization using `UBC-NLP/MARBERTv2` checkpoint.\n",
    "- Leverages `AutoTokenizer` for automatic model-specific tokenizer selection.\n",
    "- Checks for fast tokenizer (optimized for efficiency).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/anaconda3/envs/huggingface/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"UBC-NLP/MARBERTv2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] اين ولد صدام حسين ؟ [SEP] ولد صدام بن حسين بن مجيد بن عبد الغفور بن سليمان بن عبد القادر بن السيد الامير عمر الثاني امير تكريت ، ابن السيد بكر بن السيد الامير عمر الاول ، ابن السيد الامير شبيب ابن السيد الامير حسن بن السيد الامير علي ، ابن السيد الامير حسن ، ابن السيد الامير ناصر الدين العراقي - امير البصرة وامير قبايل عبادة ، يمتد انتشارها من العراق وحتى الشام – ابن السيد حسين العراقي ، ابن السيد ابراهيم العربي ، ابن السيد محمود ، ابن السيد عبد الرحمن شمس الدين ، ابن السيد عبد الله قاسم نجم الدين المبارك ، ابن السيد محمد خزام السليم ، ابن السيد شمس الدين عبد الكريم ابي محمد الواسطي ، ابن السيد صالح عبد الرزاق ، ابن شمس الدين محمد ، ابن السيد صدر الدين علي ، ابن السيد ابي علي احمد عز الدين الصياد ، ابن سيدنا القطب السيد عبد الرحيم ممهد الدولة ، ابن السيد سيف الدين عثمان ، ابن السيد حسين ، ابن السيد محمد عسلة ، ابن السيد حازم علي ابو الفوارس ، ابن السيد احمد ، ابن السيد علي ، ابن السيد رفاعه الحسن المكي نزيل المغرب ، ابن السيد المهدي ، ابن السيد ابي القاسم محمد ، ابن السيد الحسن القاسم ، ابن السيد الحسين ، ابن السيد احمد الاكبر ، ابن السيد موسى الثاني ، ابن الامام ابراهيم المرتضى ، ابن الامام موسى الكاظم ، ابن الامام جعفر الصادق ، ابن الامام محمد الباقر ، ابن الامام علي زين العابدين ، ابن الامام الحسين الشهيد ، ابن امير المومنين علي بن ابي طالب [ 11 ] [ 12 ] في قرية العوجة التي تبعد 23 كم عن مدينة تكريت الواقعة شمال غرب بغداد والتابعة لمحافظة صلاح الدين ، [ 4 ] لعايلة تمتهن الزراعة. توفي والده قبل ولادته بستة اشهر وتعددت الاقاويل التي فسرت سبب وفاته ما بين وفاة لاسباب طبيعية او مقتله على ايدي قطاع الطرق. [ 13 ] بعدها بفترة قصيرة توفي الاخ الاكبر لصدام وهو في الثالثة عشرة بعد اصابته بالسرطان. [ 14 ] [SEP]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can pass to our tokenizer the question and the context together, and it will properly insert the special\n",
    "# tokens to them ([CLS] [SEP])\n",
    "sample_idx = 14\n",
    "question = raw_dataset[\"validation\"][sample_idx][\"question\"]\n",
    "context = raw_dataset[\"validation\"][sample_idx][\"context\"]\n",
    "inputs = tokenizer(question,context) # input_ids,token_type_ids,attention_mask_ids\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] اين ولد صدام حسين ؟ [SEP] ولد صدام بن حسين بن مجيد بن عبد الغفور بن سليمان بن عبد القادر بن السيد الامير عمر الثاني امير تكريت ، ابن السيد بكر بن السيد الامير عمر الاول ، ابن السيد الامير شبيب ابن السيد الامير حسن بن السيد الامير علي ، ابن السيد الامير حسن ، ابن السيد الامير ناصر الدين العراقي - امير البصرة وامير قبايل عبادة ، يمتد انتشارها من العراق وحتى الشام – ابن السيد حسين العراقي ، ابن السيد ابراهيم العربي ، ابن السيد محمود ، ابن السيد عبد الرحمن شمس الدين ، ابن [SEP]\n",
      "[CLS] اين ولد صدام حسين ؟ [SEP] علي ، ابن السيد الامير حسن ، ابن السيد الامير ناصر الدين العراقي - امير البصرة وامير قبايل عبادة ، يمتد انتشارها من العراق وحتى الشام – ابن السيد حسين العراقي ، ابن السيد ابراهيم العربي ، ابن السيد محمود ، ابن السيد عبد الرحمن شمس الدين ، ابن السيد عبد الله قاسم نجم الدين المبارك ، ابن السيد محمد خزام السليم ، ابن السيد شمس الدين عبد الكريم ابي محمد الواسطي ، ابن السيد صالح عبد الرزاق ، ابن شمس الدين محمد ، ابن السيد صدر الدين علي ، [SEP]\n",
      "[CLS] اين ولد صدام حسين ؟ [SEP] ابن السيد عبد الرحمن شمس الدين ، ابن السيد عبد الله قاسم نجم الدين المبارك ، ابن السيد محمد خزام السليم ، ابن السيد شمس الدين عبد الكريم ابي محمد الواسطي ، ابن السيد صالح عبد الرزاق ، ابن شمس الدين محمد ، ابن السيد صدر الدين علي ، ابن السيد ابي علي احمد عز الدين الصياد ، ابن سيدنا القطب السيد عبد الرحيم ممهد الدولة ، ابن السيد سيف الدين عثمان ، ابن السيد حسين ، ابن السيد محمد عسلة ، ابن السيد حازم علي ابو الفوارس [SEP]\n",
      "[CLS] اين ولد صدام حسين ؟ [SEP] محمد ، ابن السيد صدر الدين علي ، ابن السيد ابي علي احمد عز الدين الصياد ، ابن سيدنا القطب السيد عبد الرحيم ممهد الدولة ، ابن السيد سيف الدين عثمان ، ابن السيد حسين ، ابن السيد محمد عسلة ، ابن السيد حازم علي ابو الفوارس ، ابن السيد احمد ، ابن السيد علي ، ابن السيد رفاعه الحسن المكي نزيل المغرب ، ابن السيد المهدي ، ابن السيد ابي القاسم محمد ، ابن السيد الحسن القاسم ، ابن السيد الحسين ، ابن السيد احمد الاكبر ، [SEP]\n",
      "[CLS] اين ولد صدام حسين ؟ [SEP] ، ابن السيد حازم علي ابو الفوارس ، ابن السيد احمد ، ابن السيد علي ، ابن السيد رفاعه الحسن المكي نزيل المغرب ، ابن السيد المهدي ، ابن السيد ابي القاسم محمد ، ابن السيد الحسن القاسم ، ابن السيد الحسين ، ابن السيد احمد الاكبر ، ابن السيد موسى الثاني ، ابن الامام ابراهيم المرتضى ، ابن الامام موسى الكاظم ، ابن الامام جعفر الصادق ، ابن الامام محمد الباقر ، ابن الامام علي زين العابدين ، ابن الامام الحسين الشهيد ، ابن امير المومنين علي بن [SEP]\n",
      "[CLS] اين ولد صدام حسين ؟ [SEP] السيد الحسين ، ابن السيد احمد الاكبر ، ابن السيد موسى الثاني ، ابن الامام ابراهيم المرتضى ، ابن الامام موسى الكاظم ، ابن الامام جعفر الصادق ، ابن الامام محمد الباقر ، ابن الامام علي زين العابدين ، ابن الامام الحسين الشهيد ، ابن امير المومنين علي بن ابي طالب [ 11 ] [ 12 ] في قرية العوجة التي تبعد 23 كم عن مدينة تكريت الواقعة شمال غرب بغداد والتابعة لمحافظة صلاح الدين ، [ 4 ] لعايلة تمتهن الزراعة. توفي والده قبل ولادته بست [SEP]\n",
      "[CLS] اين ولد صدام حسين ؟ [SEP] الحسين الشهيد ، ابن امير المومنين علي بن ابي طالب [ 11 ] [ 12 ] في قرية العوجة التي تبعد 23 كم عن مدينة تكريت الواقعة شمال غرب بغداد والتابعة لمحافظة صلاح الدين ، [ 4 ] لعايلة تمتهن الزراعة. توفي والده قبل ولادته بستة اشهر وتعددت الاقاويل التي فسرت سبب وفاته ما بين وفاة لاسباب طبيعية او مقتله على ايدي قطاع الطرق. [ 13 ] بعدها بفترة قصيرة توفي الاخ الاكبر لصدام وهو في الثالثة عشرة بعد اصابته بالسرطان [SEP]\n",
      "[CLS] اين ولد صدام حسين ؟ [SEP]هن الزراعة. توفي والده قبل ولادته بستة اشهر وتعددت الاقاويل التي فسرت سبب وفاته ما بين وفاة لاسباب طبيعية او مقتله على ايدي قطاع الطرق. [ 13 ] بعدها بفترة قصيرة توفي الاخ الاكبر لصدام وهو في الثالثة عشرة بعد اصابته بالسرطان. [ 14 ] [SEP]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because we will deal with long sequences with the qa task, and our model will not has this long\n",
    "# seqs, we will split the long seqs with several features\n",
    "\n",
    "inputs = tokenizer(question,\n",
    "                   context,\n",
    "                   max_length=100,\n",
    "                   truncation=\"only_second\",\n",
    "                   stride=50,\n",
    "                   return_overflowing_tokens=True)\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))\n",
    "inputs['overflow_to_sample_mapping'] # list that has the mapping each feature to its orignal context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `offset_mapping` Explained\n",
    "\n",
    "offset_mapping deals with long contexts for question answering:\n",
    "\n",
    "- It segments contexts using a tokenizer with `max_length` and `stride` for overlapping chunks.\n",
    "- `offset_mapping` is a key element that maps tokens back to their original character positions in the context.\n",
    "\n",
    "**Why is `offset_mapping` important?**\n",
    "\n",
    "- It allows pinpointing answer locations within the original context, even if parts were truncated during tokenization. This is crucial for accurate question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3599, 3735, 11627, 5030]\n",
      "[(0, 0), (0, 3), (4, 7), (8, 12), (13, 17)]\n",
      "[CLS] اين ولد صدام حسين\n"
     ]
    }
   ],
   "source": [
    "# Explaining offset_mapping\n",
    "print(inputs[\"input_ids\"][0][0:5])\n",
    "print(inputs['offset_mapping'][0][0:5])\n",
    "print(tokenizer.decode(inputs[\"input_ids\"][0][0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `overflow_to_sample_mapping` Explaination\n",
    "\n",
    "overflow_to_sample_mapping deals with long contexts for question answering by splitting them using tokenization. \n",
    "\n",
    "- `overflow_to_sample_mapping` is a list that tracks the origin of each generated feature (tokenized segment). It links each feature back to the specific question-context pair in the original dataset from which it originated.\n",
    "\n",
    "This is essential for associating model predictions on individual features with the corresponding full context in the dataset, enabling accurate answer retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4 examples gave 20 features.\n",
      "Here is where each comes from: [0, 0, 1, 1, 2, 3, 4, 4, 5, 5, 6, 7, 8, 9, 9, 9, 9, 9, 9, 9].\n"
     ]
    }
   ],
   "source": [
    "# Explaining \"overflow_to_sample_mapping\"\n",
    "inputs = tokenizer(\n",
    "    train_dataset_sel[2:12][\"question\"],\n",
    "    train_dataset_sel[2:12][\"context\"],\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code processes question-answer pairs in a long context QA setting. It aims to predict answer locations within the original context.\n",
    "\n",
    "1. **Leveraging `overflow_to_sample_mapping`:**\n",
    "\n",
    "   - It links each tokenized feature (generated due to long context splitting) back to its original question-answer pair in the dataset.\n",
    "\n",
    "2. **Extracting Answer Locations:**\n",
    "\n",
    "   - It iterates through features and their origins to find corresponding answer objects.\n",
    "   - It checks if the answer falls entirely within the current context slice based on character positions and offset information.\n",
    "   - If inside the context, it pinpoints the answer's starting and ending token positions within the feature.\n",
    "\n",
    "3. **Output:**\n",
    "\n",
    "   - The code generates lists `start_positions` and `end_positions` that indicate the predicted answer locations within the original contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sticking all together\n",
    "answers = train_dataset_sel[2:12][\"answers\"]\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
    "    answer = answers[sample_idx]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label is (0, 0)\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: مادة كيميائية تتفاعل في الدماغ لتؤثر على كثير من الأحاسيس والسلوكيات بما في ذلك الانتباه, labels give: مادة كيميايية تتفاعل في الدماغ لتوثر على كثير من الاحاسيس والسلوكيات بما في ذلك الانتباه\n"
     ]
    }
   ],
   "source": [
    "# Checking our code if it gives us the same answer as a label\n",
    "idx = 10\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
    "\n",
    "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a function named `preprocess_training_examples` specifically designed for processing question-answer training data in a long context setting:\n",
    "\n",
    "**1. Data Cleaning:**\n",
    "   - Uses `araby.strip_diacritics` to remove diacritics (Tashkeel) from both questions and contexts.\n",
    "\n",
    "**2. Tokenization with Context Splitting:**\n",
    "   - Leverages the `tokenizer` (presumably loaded with `UBC-NLP/MARBERTv2`) to tokenize questions and contexts.\n",
    "   - Sets `max_length` to limit the number of tokens per feature (segment).\n",
    "   - Uses `truncation=\"only_second\"` to truncate the context only.\n",
    "   - Sets `stride` to create overlapping segments of the context for better coverage.\n",
    "   - Enables `return_overflowing_tokens=True` to handle cases where the context exceeds `max_length`.\n",
    "   - Enables `return_offsets_mapping=True` to track character positions within the original context.\n",
    "   - Employs `padding=\"max_length\"` to ensure all features have the same length (padded with special tokens if necessary).\n",
    "\n",
    "**3. Extracting Answer Locations:**\n",
    "   - Extracts `offset_mapping` and `overflow_to_sample_mapping` from the tokenizer outputs.\n",
    "   - Iterates through each feature's `offset_mapping` and corresponding `sample_map` entry.\n",
    "      - `offset_mapping` provides character positions within the context for each token in the feature.\n",
    "      - `sample_map` helps identify the original question-answer pair this feature originated from.\n",
    "   - Retrieves the corresponding answer object from `answers` based on the `sample_idx` from `sample_map`.\n",
    "   - Locates the start and end character positions of the answer within the original context.\n",
    "   - Analyzes token positions within the feature using `sequence_ids` to determine the context slice.\n",
    "   - Checks if the answer falls entirely within the current context slice based on character positions and offset information.\n",
    "   - If the answer is inside the context, it pinpoints the starting and ending token positions within the feature for that specific answer.\n",
    "   - If the answer is outside the context, it assigns labels of (0, 0) to indicate no answer found.\n",
    "\n",
    "**4. Output:**\n",
    "   - The function modifies the original `inputs` dictionary by adding two new keys:\n",
    "      - `start_positions`: a list containing predicted starting token positions of the answer within each feature.\n",
    "      - `end_positions`: a list containing predicted ending token positions of the answer within each feature.\n",
    "   - The preprocessed data (including tokenized text, attention masks, padding information, and predicted answer locations) is returned as the function's output `['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    # removing diacritics from our context and queestions\n",
    "    questions = [araby.strip_diacritics(q.strip()) for q in examples[\"question\"]]\n",
    "    contexts = [araby.strip_diacritics(c) for c in examples[\"context\"]]\n",
    "    # seting up the tokenizer\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i] # context 0 -> feature 1, feature 2, feature 3 -> [0, 0, 0],, we get the context that this feature belongs to\n",
    "        answer = answers[sample_idx] # getting the answer of the context[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c278bc1bd4b4a2e858605530d374cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14805 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applying our function to the train dataset\n",
    "train_dataset = train_dataset_sel.map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset_sel.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 15379\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49881, 15379)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare our arabic dataset with original dataset\n",
    "len(raw_dataset[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code defines `preprocess_validation_examples` for validation data in long context QA. Here's a summary:\n",
    "\n",
    "- **Data Cleaning:** Similar to training, it removes diacritics for normalization.\n",
    "- **Tokenization:** It performs tokenization with the same settings as training for consistency. It enables features for long contexts and tracking character positions.\n",
    "- **Preserving Example IDs:** It extracts the original example ID for each feature (where it originated from in the dataset) using `sample_map`.\n",
    "- **Context-Aware Offset Mapping:** It adjusts `offset_mapping` by setting offsets to `None` for tokens outside the actual context (using `sequence_ids`). This likely avoids using irrelevant character positions during evaluation.\n",
    "- **Output:** It modifies `inputs` to include `example_id` (original IDs) and potentially adjusted `offset_mapping`. The preprocessed validation data is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [araby.strip_diacritics(q.strip()) for q in examples[\"question\"]]\n",
    "    contexts = [araby.strip_diacritics(c) for c in examples[\"context\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5f09dacd2c4169919728e7fe799c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/921 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(921, 940)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = val_dataset_sel.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset_sel.column_names,\n",
    ")\n",
    "len(val_dataset_sel), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 940\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] كم عدد مرات فوز الاوروغواي ببطولة كاس العالم لكرو القدم ؟ [SEP] اقيمت البطولة 21 مرة ، شارك في النهاييات 78 دولة ، وعدد الفرق التي فازت بالبطولة حتى الان 8 فرق ، ويعد المنتخب البرازيلي الاكثر تتويجا بالكاس حيث فاز بها 5 مرات اعوام : 1958 ، 1962 ، 1970 ، 1994 و2002. يليه المنتخب الايطالي الذي احرزها 4 مرات في اعوام : 1934 ، 1938 ، 1982 و2006 ، بالمشاركة مع المنتخب الالماني الذي حققها 4 مرات ايضا اعوام : 1954 ، 1974 و1990 و2014 ، ثم الاوروغواي والارجنتين وفرنسا برصيد بطولتين. بينما احرزت منتخبات انجلترا واسبانيا البطولة مرة واحدة. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# picking first sample from validation set\n",
    "tokenizer.decode(validation_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/anaconda3/envs/huggingface/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd112b8f5e97412884bc35823dac066f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/757 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26f4c9b1b8a4eb6aa0054b972778b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/654M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at UBC-NLP/MARBERTv2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Loading the pre-trained MARBERTv2 to fine tune it\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the training arguments:\n",
    "\n",
    "- **Output Directory (\"MARBERT-finetuned-tydiqa\")**: Stores model checkpoints, evaluation outputs, and training logs.\n",
    "- **No Automatic Evaluation**: Disables evaluation during training.\n",
    "- **Save Checkpoints per Epoch**: Saves the model state at the end of each training epoch.\n",
    "- **Learning Rate (2e-5)**: Controls how much the model updates based on the loss (common starting point for fine-tuning).\n",
    "- **Number of Epochs (3)**: Defines how many times the entire training dataset is passed through the model.\n",
    "- **Weight Decay (0.01)**: Regularization technique to prevent overfitting by penalizing large model weights.\n",
    "- **Mixed Precision (FP16=True)**: Enables potentially faster training on GPUs with appropriate hardware support.\n",
    "- **No Hub Upload (push_to_hub=False)**: Disables pushing the fine-tuned model to the Hugging Face Hub model repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"MARBERT-finetuned-tydiqa\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5769' max='5769' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5769/5769 22:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.939700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.656300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.651400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.600100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.511500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.308500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.272500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5769, training_loss=1.7626281198708886, metrics={'train_runtime': 1337.0989, 'train_samples_per_second': 34.505, 'train_steps_per_second': 4.315, 'total_flos': 9041586349146624.0, 'train_loss': 1.7626281198708886, 'epoch': 3.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.4497292935848236,\n",
       " 'start': 2,\n",
       " 'end': 31,\n",
       " 'answer': 'أحمد مازن أحمد أسعد الشقيري ('}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"MARBERT-finetuned-tydiqa/checkpoint-5769\"\n",
    "question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n",
    "\n",
    "context = \"\"\"\n",
    "\n",
    "أحمد مازن أحمد أسعد الشقيري (19 يوليو 1973م، جدة) إعلامي سعودي من أصول فلسطينية[1][معلومة 1] بدأ بتقديم برامج فكرية اجتماعية ومضيف السلسلة التليفزيونية خواطر والمضيف السابق لبرنامج يلا شباب، ألّف برامج تلفازية حول مساعدة الشباب على النضج في أفكارهم والبذل في خدمة إيمانهم وتطوير مهاراتهم واكتشاف معرفتهم بالعالم وبدورهم في جعله مكاناً أفضل.[2] اشتهر الشقيري في السعودية والوطن العربي بعد سلسلة برنامج خواطر التي حققت نجاحاً واسعاً نتيجة بساطة أسلوبها ومعالجتها لقضايا الشباب والأمة والتي كانت دائماً تبدأ بمقولته\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "question = \"من مقدم برنامج خواطر؟\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  
